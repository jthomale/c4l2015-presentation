
You Gotta Keep 'em Separated
  The Case for "Bento Box" Discovery Interfaces

  
I'm Jason Thomale, I'm the Resource Discovery Systems Librarian at the University of North Texas. I'm here to make the case for "bento box" discovery interfaces.

------

So, I submit to you the following.

------

First the obligatory, here's where the "bento box" UI metaphor comes from. A Japanese lunch where

------

several different types of delicious food

------

are subdivided into little compartments.

------

Bento box.

------

And so here we have a search results UI where

------

results are subdivided

------

into compartments by category,

------

like a Japanese bento box. This is from University of Michigan Libraries. Which is great and all, but isn't this just another in a long line of library hacks and workarounds?

------

Well, last I'll show you another search results UI where, again,

------

results are subdivided

------

into compartments by category.

------

Bento box ...

------

[next next next]

from Google!

------

Questions? [Hopefully pause for laughter] But seriously--I do have to fill 20 minutes, here...

------

------

So, the thing about talking about this at Code4lib is that the Code4lib community is professionally pretty diverse. And when we're talking about bento box interfaces,

------

we have a distinct bias toward academic libraries.

------

I mean that *is* where the term originated and, as far as I know, that's mainly the context where it's used. I think the accepted story is that NCSU had the first bento box interface, and Tito Sierra (at NCSU at the time) coined the term to describe it.

And--especially in academic libraries, we have sort of a collective history of implementing what we might call 

------

"hacks," especially when trying to facilitate a particular user experience on our websites. I don't mean this disparagingly, by the way, as we're all pretty much in this boat. We often have to get

------

different systems from different vendors to play nicely together.

------

Sometimes we need to customize things that aren't easily customizable. 

------

Sometimes we need to try to get data out of systems that just don't want to give it up.

------

Sometimes we just need to make things work together that weren't meant to work together.

------

Granted, things are better than they used to be. Open source software in libraries is thriving, and it's become more or less the norm for vendors to offer APIs or other methods of getting at our data. 

------

And so we build better and more sophisticated hacks--like this lovely treadmill bike. Or bento box discovery interfaces.

But here's the thing; as I look at resource discovery solutions in libraries and outside of libraries,

------

I'm seeing bento box as an interesting type of UI for more reasons than just the solutions it provides for the immediate problems it was designed to work around. While it's true that those problems are real for academic libraries and bento box UIs are still attractive to us as a solution to those problems, I'm becoming more convinced that there's more to it than that.

------

But this is mainly what I'm going to be talking about, as I have some stats to share that might be relevant. So let's delve into "bento box UI as hack." What

------

are the actual problems that it was meant to solve?

------

To answer that, we have to get into the story of library discovery.

------

And I really tried to think of a way to start this next part that didn't involve telling this story, because it's one we know so damn well. But, just in case, I'll refresh our memories and make sure we're all up to speed. (Forgive the oversimplification.)

------

Library users have repeatedly told us: 

------

Google--

------

good, easy to use!

------

library sites--

------

bad, hard to use.

------

So we've said: let's make 

------

library sites 

------

more like

------

Google

------

Yeah!

------

And what *is* Google?

------

A single search box that 

------

searches a Web index 

------

and presents results in a relevance-ranked list. Right?

------

And what are library systems?

------

Information silos! Many data stores, impossible to search at once, 

------

users have to know which ones to use or how to find out which ones to use,

------

interfaces are all different, searching takes forever.

------

So we said:

------

well okay, let's try taking a user's 

------

search and sending it out 

------

to a bunch of databases for them at once. 

------

Let's even bring the results together on the fly into one list! 

------

Yeah! So much like Google!

------

Nope. Not like Google. Too slow. Way too many possible endpoints and we can't search them all at once. No way to rank results effectively.

-------

So we said:

-------

all right, what we need is a central index. That's what Google has.

-------

Some of us started putting stuff into Solr.

-------

But just stuff we could get. 

-------

And we put search interfaces on top. 

-------

Yeah! Next Generation Catalogs! So awesome!

-------

Users said: but where's the articles?

-------

So we said: 

-------

Fine. Let's make 

-------

deals with publishers and content providers,

-------

and build a big ass central index of licensed content. (Sorry--vendors only.) 

-------

:-( 

-------

And let's put the catalog and other stuff in it too for good measure.

-------

Yeah! Web scale discovery!

*Now* are we like Google?

-------

Let's see. Single index for our stuff?

-------

Check. Single relevance-ranked results list?

-------

Check. Single search box? 

-------

Check.

Okay. So...yeah. 

-------

A lot like Google. Right?

<next> <next>

-------

Meh. The similarities are skin deep. What is it *really* that makes Google Google? Is it the single index search? Is it the Google interface? Is it the Google style of search results?

No, it's the Web!

More than that, it's how Google harnessed the unique properties of the Web--the hyperlinks, the loosely structured, openly-available data stored in consistent document formats--and figured out how to do solid relevance ranking for Web searches. *That's* what Google search is all about. That's what made them the best, and that's why people use them. We KNOW this.

So, at the heart of Google's success is their relevance ranking, and at the heart of that is the Web itself. The various conglomerations of resources that libraries offer are not the Web. And what is it that Web Scale Discovery systems have always had trouble with?

Yup. Relevance ranking.

So yeah users tell us they want us to be like Google because Google gets them to the information they want with little fuss. And, on our library websites, when trying to figure out how we want to direct users to find our resources, that's exactly what we're trying to do. If we funnel people to a system that looks, feels, and acts like Google but then doesn't *actually* return the most relevant results, are we really doing what they want, no matter how satisfied they seem to be?

Bento box UI design in libraries is mainly an attempt to address this, to get people to the most relevant information faster and more easily than other approaches (including combined-results discovery).

1. It gives more equal weight to resources from multiple systems that might otherwise be completely drowned out in a single results list. This helps bring a wider variety of things to users' attention and helps them get a better idea of what's available with less effort.

2. It gets around the problem of library system silos by providing one place to search and then quickly funnelling people to the resources they need in the particular systems that natively store and provide services over those resources.

3. It attempts to accommodate multiple and perhaps incompatible uses by separating results by resource type. Articles, books, databases, etc. are all different types of resources that have different uses. Separating results visually helps users zero in on the resource type they need more quickly. They don't have to futz around with filters and facets that they tend not to use anyway.

From a strategic POV, bento box design also has benefits: for one, libraries maintain more control over our primary search interface, relegating vendor interfaces to components that can be more switched out. (Of course this is only a benefit if you have the staff and expertise to build and maintain that interface. Otherwise it's probably a drawback.)

So, I'm standing here talking to you like all of this is obvious, and you'd maybe assume that there's clear evidence to back me up that, number 1, the problems that bento box UIs were designed to solve actually exist and, number 2, that bento box UIs actually solve them. But if clear evidence existed, then we'd probably all have bento box UIs by now, and clearly we don't. In reality, the evidence, where it exists, is murky.

To be honest it seems that many user studies that have been done have shown that users do seem satisfied by the combined-results approach, primarily undergraduates. They like it for the same reasons they like Google: it's fast, easy, and it gets them *some* sort of results quickly. The relevance ranking, though not on par with Google, doesn't exactly suck, either. And so, maybe more often than not, it does work and it does satisfy users' needs (for some types of users and some types of needs).

But other user studies suggest that most users come to the library interface knowing at a very broad level what type of resource they need to get (like articles vs books) and so they find results that blend them together confusing. They use different types of resources differently, and in a blended interface it takes effort for them to filter stuff because the relevance ranking doesn't do a good enough job of it.

I don't actually know how to reconcile these. But--the main reason I'm here talking to you today is because I might have something to add to the picture. Recently I completed an analysis of usage statistics of our discovery interfaces at UNT as they've evolved over the past 3 years. And it kind of hit me as being pretty good evidence to add to the "bento box" camp, although it maybe isn't as clear and unequivocal as we might want. It does support some things other libraries have found and maybe (hopefully) adds a little bit to the body of knowledge.

A little background. I've been at UNT for 4 years. During that time part of my job has been to help guide the planning and discussion about how we improve the resource discovery experience on our website. There's been tons internal debate, discussion, and research to help us try to figure out what actually would best serve our users. And we've been playing it conservatively. Here's a summary of relevant changes we've made.

In 2011 we redesigned our library catalog, which went live in September that year. We acquired and then ran Summon as a "beta" during Spring 2012. We opted not to put our catalog into Summon and basically treat it as an "articles" search, since that's what our user data suggested we most needed. We made Summon live at the same time we redesigned our website (September 2012). And, in the redesign, we featured a tabbed search box front and center (which we call the "Find box") and presented "Articles" and "Books and More" as the first two tabs. Articles was the default tab and searched Summon (limited to the Articles content type), and Books and More searched the catalog. Once our redesign went live, we really didn't touch anything for a couple of years, and we ran Google Analytics on both systems to collect stats.

So by September 2014 we'd accumulated 3 years of data on our catalog and over 2 years of data on Summon. At that point we were already in the planning stages on a bento box design, and I wanted to see if the data held any useful information about how people used our existing discovery systems. So that's when I did my analysis.

The primary question I was interested in answering was: do our users actually seem to use articles differently than they use books and other catalog materials? To answer that I needed to prove two things: one, that people *are* mainly using Summon as an articles search and they *are* mainly using the catalog to find books, media, etc.; and two, that the use patterns we see between those two systems is different enough to suggest different use cases for those types of materials.

To conduct the analysis, I first wrote Python scripts that pull data from each of our Google Analytics accounts--Summon and the catalog--using the GA API, for the period of August 2011 to August 2014. I focused on two different types of stats: pageviews, which I used as an approximation for number of searches, and search queries.

In order to help force pageviews into a useful approximation of searches (and to serve as a useful point of comparison), I filtered them by URL pattern. Anything that was not a hit on the first page of a search results screen got filtered out. Hits where a user changed search terms or used facets did not get filtered out and counted as additional searches. Views on search results screens that came from external sources--such as bookmarks--were also counted.

I used additional GA filters to look at different aspects of usage in each system, including the following:

Aggregate patterns.
Distance usage.
Usage coming from the Find Box on the homepage compared to direct usage.
Usage of search options on the Find Box.
Usage of search options such as index and collection searches in the catalog.
Usage of search options such as facets and filters in Summon.

With search queries I attempted to do some rough query analysis, although dealing with the sheer amount of data is tough without some advanced natural language processing chops. I mainly looked at the top 100 queries in each system--top 100 queries total and top 100 entered via the Find Box. I also attempted to normalize queries (where I did basic case and punctuation normalization and removed stop words), and then I looked at the top 100 individual words, top 100 bigrams, and top 100 trigrams in each data set. Finally, I looked at the distribution of queries, words, bigrams, and trigrams in each system, and I looked at query length.

All right, so--no surprise here, but, generally speaking, I think my findings confirm that people *do* use these two systems differently and that the assumption that they understand they're working with different resource types in each one is valid--enough, I think, to help make an argument for bento box UI design.

We'll start with aggregate stats for each system. Here's where we implemented Summon as a beta service, and here's where that went live. So during this entire period, Summon and the catalog were both live, searchable via different tabs in our Find box. We didn't make any changes to either interface during this time that would have much of an effect on these stats.

In terms of absolute numbers, usage is almost equal. The catalog is still a little bit higher, but not by much. When we first implemented Summon, one thing that we hypothesized was that maybe Summon would completely cannibalize our catalog use--and that just hasn't happened. We clearly see a fairly sizeable drop in catalog use as we ramped up Summon implementation--but we don't see a wholesale switch from one system to the other.

Next, we see two clearly different patterns of use in each system by semester. Here's fall, spring, fall, spring. There's more search activity in the catalog at the beginning of the semester, and then it drops off by the end. In Summon, we see the opposite pattern. Usage at the beginning of the semester is low, and it peaks near the end. (And I want to note that this is very similar to a pattern that NCSU Libraries also noticed in their systems, which they discuss in their paper, "How Users Search the Library from a Single Search Box.")

Last thing I want to point out about this. Troughs that happen between semesters are deeper and wider in Summon than in the catalog. When a semester ends, Summon use drops off immediately and remains low until the next semester begins, while catalog use remains a little bit steadier during semester breaks. (With the exception of Christmas/New Years.)

Now let's look at distance usage. This shows searches in each system using two different metrics for "distance." It compares all searches (the red line) with searches coming from Denton, TX (where UNT is located, the blue line) and searches coming from the unt.edu domain ("on campus", the green line). Either way we slice it, we see that the catalog gets more of its use from local users and Summon gets more of its use from distance users.

Now let's look at usage of these two systems by where users started their search session--how they ended up in the system they ended up in. Here we see that 75.34% of search sessions in Summon start from the Find Box, while only 40% of catalog search sessions do. Catalog search sessions are almost as likely to start from the catalog home page. Origination from other sources is also pretty high, I think because we have some other entrance points like course reserves, a scoped media search, and a scoped music search, which all see a fair amount of use.

Okay next let's talk about queries.

We see that the top queries in the catalog tend to be searches related to video games, films, television, and books. We also see a lot of searches in the catalog for particular resources--reference sources and serials especially. Looking at the top words, bigrams, and trigrams causes some music, literature, and history related terms and concepts to emerge.

In Summon, on the other hand, we mostly see queries related to academic topics or current events.

To be fair, we do see some similarities between the two systems as well. In both systems we see things like...naxos, lexis nexis, psycinfo, ebsco, and jstor. We also see some of the same terms cropping up in the top queries for both systems--some things you might expect like "higher education" and "university north texas" and some things you might not like "social media" and "human resource management."

And I also want to reiterate that, when I looked at queries, I also made sure to analyze JUST the queries coming from the Find Box both for Summon and the catalog in addition to the overall queries for each system. Although some of the search terms were different, I did see the same patterns.

Query analysis of course is tricky because there's a lot of data and a lot of variation. Looking at words, bigrams, and trigrams was one way to try to normalize and group queries. I've also attempted to characterize queries in two other ways: by looking at overall query length and by looking at search frequency distribution of the top 100 queries as a cumulative percentage of total searches.

So, this graph shows query length, looking at queries broken out by system and by Find Box vs all searches. We see that searches in the catalog tend to be shorter and searches in Summon tend to be longer. This jives with what others have found and I think maybe supports the idea that one prominent use case for Summon is to copy/paste an article title or citation to conduct known item searches.

This next set of graphs was an attempt to look at what percentage of total searches the top 100 queries, words, bigrams, and trigrams represent for each system. So: are the top 100 queries noticeably more heavily searched in one system compared to another?

And what we see is: for everything except single words, there is a noticeable--but not huge--difference between the catalog and Summon. The top 100 queries, or bigrams, or trigrams, that we see in the catalog represent a higher percentage of the total number of queries (bigrams, trigrams, etc.) than the top 100 queries etc. in Summon. This says to me that the distribution of searches in the catalog is slightly more heavily weighted toward the top queries, so people are tending to search a smaller set of queries more frequently in the catalog compared to Summon. And this pattern is even more pronounced when you look at the catalog queries entered into the Find Box.

All right. Sort of the last piece of the puzzle I want to complete is how people interact with Summon. I've mentioned that we primarily use Summon as a "find articles" search in our discovery environment. That search actually limits to the "journal article" content type by default. And in my analysis here, I'm basically equating use of Summon with use of articles. Is that a safe assumption, at least the majority of the time?

This is actually a little hard to answer because of the simple fact that people hardly ever change default settings. We see this over and over again in all kinds of UI research. So, in *our* stats, if we see that people rarely search things other than articles, is it because articles are actually what they want, or is it that they just don't want to bother changing the default?

Well, let's see what we see. Within Summon, 59.22% of searches never changed the default facets or filters at all, and 26.04% did. 8.26% of searches represent a new basic search (where facets/filters are completely reset), and 6.47% of searches were a new advanced search. Note that these categories are mutually exclusive, and that changes to facets/filters after a default Articles search or after a basic or advanced search are included in the 26.04% figure. So some of those facet/filter changes represent people adding the default filters back after they conducted a basic or advanced search that cleared out the default filters. (This is important to keep in mind.)

When we look at the types of facets and filters that are actually used, we see that changing content type is fifth, at 9.23% of facet/filter use. And of those, we're interested in seeing how people interact with journal articles. The top content type interaction is to add a limit for Journal Articles, at 21.94% of all content type interactions. The third is to remove the limit for Journal Articles, at 8.78%. Not even shown here because its use is so low is the Exclude Journal Articles filter, at 0.54%. Only 0.24% of total searches explicitly remove the Journal Articles content type limit, and 0.01% of total searches explicitly excluded journal articles. Finally, if we only look at the activity where people added a content type limit (as opposed to removing a limit or adding or removing an exclusion), we see that adding the Journal Articles limit accounts for 41.27% of all of that activity.

So here's what we can say for sure about this.

Number one, as we figured, people mostly don't mess with changing the defaults. (59.22% of total searches.)
Number two, even when people do change facets/filters, they rarely change content type. (1.89% of total searches.)
Number three, even when people do facets/filters, they rarely explicitly remove the Journal Article limit. (0.21% of total searches and only 0.91% of all facet/filter changes.)
Number four, people almost never actually exclude Journal Articles. (0.01% of total searches and 0.06% of all facet/filter changes.)
Number five, when people do change content type, adding the Journal Article content type limit back into their search is what we see most often. In absolute terms this is still rare, however you have to keep in mind that, since that limit is present on most searches by default, the opportunity to add the limit is a lot rarer than the opportunity to remove it or add other content type limits. 41.27% of all 

I think, while we can't say based on these numbers that people use Summon *exclusively* for searching articles, I think it does validate our assumption that people *primarily* use Summon for articles.

Okay let's wrap this up.

What we in libraries specifically refer to as the bento box interface basically started out as a hack to work around the particular aspects of the current system ecology that we find ourselves in. In most cases we're putting results from different systems into their own boxes, and we're shoehorning those results into categories by resource types that make sense to our users and best match up with what these systems contain. We're trying to create the illusion of one overarching library system and one consistent user experience instead of the pieced-together Frankenstein monsters that most of us actually have. Depending on our implementation, the places where the boundaries between systems don't quite line up with the categories we use in the UI I think is where we still have some problems in terms of meeting user expectations.

But the ultimate problems we're trying to solve with bento box UI design, once you get beyond the hackyness of any given implementation, I think are completely real and more universal than we might think. In the usage stats I've looked at, we see evidence that people do search for and use different types of resources differently. While we want to offer people the ease of a single-search box, we also want to make sure we don't accidentally make the things they want more difficult to find. Over the past couple of years, we've also been seeing the major search engines including more and more what we'd call bento box characteristics in their UIs. When Serials Solutions released Summon 2.0, we saw some of these same characteristics in their interface as well. Obviously there are differences in the way search engines implement these characteristics and the way that libraries typically have, but it seems to be the same attempt at bringing the most relevant and the widest variety of potentially relevant material to the top where users are likely to see it.




















