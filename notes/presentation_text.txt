
You Gotta Keep 'em Separated
  The Case for "Bento Box" Discovery Interfaces

  
I'm Jason Thomale, I'm the Resource Discovery Systems Librarian at the University of North Texas. I'm here to make the case for "bento box" discovery interfaces.

------

So, I submit to you the following.

------

First the obligatory, here's where the "bento box" UI metaphor comes from. A Japanese lunch where

------

several different types of delicious food

------

are subdivided into little compartments.

------

Bento box.

------

And so here we have a search results UI where

------

results are subdivided

------

into compartments by category,

------

like a Japanese bento box. This is from University of Michigan Libraries. Which is great and all, but isn't this just another in a long line of library hacks and workarounds?

------

Well, last I'll show you another search results UI where, again,

------

results are subdivided

------

into compartments by category.

------

Bento box ...

------

[next next next]

from Google!

------

Questions? [Hopefully pause for laughter] But seriously--I do have to fill 20 minutes, here...

------

------

So, the thing about talking about this at Code4lib is that the Code4lib community is professionally pretty diverse. And when we're talking about bento box interfaces,

------

we have a distinct bias toward academic libraries.

------

I mean that *is* where the term originated and, as far as I know, that's mainly the context where it's used.

And--especially in academic libraries, we have sort of a collective history of implementing what we might call 

------

"hacks," especially when trying to facilitate a particular user experience on our websites. I don't mean this disparagingly, by the way, as we're all pretty much in this boat. We often have to get

------

different systems from different vendors to play nicely together.

------

Sometimes we need to customize things that aren't easily customizable. 

------

Sometimes we need to try to get data out of systems that just don't want to give it up.

------

Sometimes we just need to make things work together that weren't meant to work together.

------

Granted, things are better than they used to be. Open source software in libraries is thriving, and it's become more or less the norm for vendors to offer APIs or other methods of getting at our data. 

------

And so we build better and more sophisticated hacks--like this lovely treadmill bike. Or bento box discovery interfaces.

But here's the thing; as I look at resource discovery solutions in libraries and outside of libraries,

------

I'm seeing bento box as an interesting type of UI for more reasons than just the solutions it provides for the immediate problems it was designed to work around. While it's true that those problems are real for academic libraries and bento box UIs are still attractive to us as a solution to those problems, I'm becoming more convinced that there's more to it than that.

------

But this is mainly what I'm going to be talking about, as I have some stats to share that might be relevant. So let's delve into "bento box UI as hack." What

------

are the actual problems that it was meant to solve?

------

To answer that, we have to get into the story of library discovery.

------

And I really tried to think of a way to start this next part that didn't involve telling this story, because it's one we know so damn well. But, just in case, I'll refresh our memories and make sure we're all up to speed. (Forgive the oversimplification.)

------

Library users have repeatedly told us: 

------

Google--

------

good, easy to use!

------

library sites--

------

bad, hard to use.

------

So we've said: let's make 

------

library sites 

------

more like

------

Google

------

Yeah!

------

And what *is* Google?

------

A single search box that 

------

searches a Web index 

------

and presents results in a relevance-ranked list. Right?

------

And what are library systems?

------

Information silos! Many data stores, impossible to search at once, 

------

users have to know which ones to use or how to find out which ones to use,

------

interfaces are all different, searching takes forever.

------

So we said:

------

well okay, let's try taking a user's 

------

search and sending it out 

------

to a bunch of databases for them at once. 

------

Let's even bring the results together on the fly into one list! 

------

Yeah! So much like Google!

------

Nope. Not like Google. Too slow. Way too many possible endpoints and we can't search them all at once. No way to rank results effectively.

------

So we said: all right,

------

what we need is a central index. That's what Google has.

------

Some of us started putting stuff into Solr.

------

But just stuff we could get. 

------

And we put search interfaces on top. 

------

Yeah! Next Generation Catalogs! So awesome!

------

Users said: but where's the articles?

------

So we said: 

------

Fine. Let's make 

------

deals with publishers and content providers,

------

and build a big ass central index of licensed content.

------

And let's put the catalog and other stuff in it too for good measure.

------

Yeah! Web scale discovery!

*Now* are we like Google?

------

Let's see. Single index for our stuff?

------

Check. Single relevance-ranked results list?

------

Check. Single search box? 

------

Check.

Okay. So...yeah. 

------

A lot like Google. Right?

<next> <next>

------

Meh. The similarities are skin deep.

------

Look, what is it *really* that makes Google Google? 

------

Is it the Google interface?

------

the single index search?

------

the Google style of search results?

------

No--

------

it's the Web itself.

------

More than that, it's how Google moved beyond full-text indexing 

------

and actually harnessed the unique properties of the Web--

------

the hyperlinks, 

------

the loosely structured, 

------

openly-available data stored in 

------

standard

------

machine-readable document formats--and figured out how to do 

------

crazy good relevance ranking for Web searches. *That's* what made them the best, and that's why people use them.

------

And library data just doesn't have the same characteristics as Web data. Though I know lots of folks working on Web Scale Discovery have made big strides, nobody has *solved* relevance ranking for full library discovery the way Google solved it for the Web.

------

So yeah users tell us they want us 

------

to be like Google 

------

because Google 

------

gets them to the information they want with little fuss. 

------

And, on our library websites, when we're trying to figure out how we want to direct different user groups with various and possibly incompatible tasks to find the resources they need to solve their tasks, that's what we're trying to do, too. 

What we don't want 

------

is to funnel people to a system that looks, feels, and acts like Google but then 

------

fails to return the most relevant results for major categories of user tasks.

------

So Bento box UI design is a workaround for this problem. It's mainly an attempt by libraries to deliver a single search that works better across a wider variety of user scenarios, given the realities of library systems and data. There are at least three things that it does.

------

It gives more equal weight to resources from multiple systems that might otherwise drown each other out in a single results list. This helps bring a wider variety of things to users' attention and helps them get a better idea of what's available with less effort.

------

It tries to get around the problem of library system silos by providing one place to search and then quickly funnelling people to the resources they need in the particular systems that natively store and provide services over those resources.

------

And it attempts to accommodate multiple and perhaps incompatible uses by separating results by resource type. Articles, books, databases, etc. are all different types of resources that have different uses. Separating results visually helps users zero in on the resource type they need more quickly. They don't have to futz around with filters and facets that they tend not to use anyway.

------

From a strategic POV, bento box design also has benefits: mainly that libraries maintain more control over our primary search interface,

------

relegating vendor interfaces to components that can be more readily switched out. (Of course you have to have adequate technical staff and expertise to take advantage of this benefit.)

------

So. I'm standing up here talking to you like all of this is obvious, and you'd maybe assume that there's clear evidence to back me up that, number 1, 

------

the problems these UIs were designed to solve actually exist and, number 2, 

------

that these UIs actually solve them.

------

The truth is that the evidence isn't clear, especially when just looking at the user studies that have been published.

------

On the one hand you have the many user studies that have been done showing high user satisfaction with combined-results systems, primarily undergraduates.

------

And on the other hand you have user studies suggesting that most users come to the library interface knowing at a very broad level what type of resource they need to get (like articles vs books) and so results that blend them together are confusing.

------

I don't have the answers, but I might have something useful to add to the picture. 

------

Yes, that means it's almost time for stats show-and-tell.

------

First a little background. And yes, this is stuff you need to know.

------

I've been at UNT for 4 years. During that time part of my job has been to help 

------

guide the planning and discussion about how we improve the resource discovery experience on our website. There's been tons internal debate, discussion, and research to help us try to figure out what actually would best serve our users. And we've been playing it conservatively. Here's a summary of the most relevant changes we've made since I've been at UNT.

------

In 2011 we redesigned our library catalog, which went live in September of that year. 

------

We acquired and then ran Summon as a "beta" during Spring 2012. We opted not to put our catalog into Summon and basically treat it as an "articles" search, since that's what our user data suggested we most needed.

------

In September 2012, we released a website redesign and also made Summon live. In the redesign, we featured 

------

a tabbed search box front and center (which I'll refer to as the "Find box") and presented "Articles" and "Books and Media" as the first two tabs. Articles was the default tab and searched Summon (limited to the Articles content type), 

------

and Books and Media searched the catalog. Once our redesign went live, we didn't touch anything big for a couple of years, 

------

and we ran Google Analytics on both systems to collect stats.

------

So by September 2014

------

we'd accumulated 3 years of data on our catalog and over 2 years of data on Summon. By then we were already in the planning stages on a bento box design, 

------

and I wanted to see if the data held any useful information about how people used our existing discovery systems. So that's when I did my analysis.

------

The primary question I was interested in answering was: 

------

do our users actually seem to use articles differently than they use books and other catalog materials? And to answer that it seemed I needed to prove two things: one, that, in our environment, 

------

people *are* mainly using Summon as an articles search and

------

they *are* mainly using the catalog to find books, media, etc. And two 

------

that the use patterns we see between those two systems is different enough to suggest different use cases for those types of materials.

------

To conduct the analysis, I wrote Python scripts that 

------

pull data from each of the two Google Analytics accounts--Summon and the catalog--using the GA API, for the period of 

------

August 2011 to August 2014. I focused mainly on two different types of stats: 

------

pageviews and search queries.

------

I used pageviews as an approximation of searches. In order to do this and ensure I was comparing apples to apples in each system, I filtered by URL to remove anything that was not a hit on the first page of a search results screen. Hits where a user changed search terms or used facets counted as additional searches, and views on search results screens that came from external sources--such as bookmarks--were also counted.

I used additional GA filters to look at different aspects of usage in each system, including the following:

------

Aggregate patterns.

------

Distance usage.

------

Usage coming from the Find Box on the homepage compared to direct usage.

------

And usage of search options, including use of the options on the Find Box as well as usage of search options within each system--index and collection searches in the catalog and facets and filters in Summon.

------

With search queries I attempted to do some rough query analysis, although dealing with the sheer amount of data is tough without some advanced natural language processing chops. 

I mainly looked at 

------

the top 100 queries in each system--

------

top 100 queries overall and 

------

top 100 entered via the Find Box. I also attempted 

------

to normalize queries (where I did case and punctuation normalization and removed stop words), and then I looked at the top 100 

------

individual words, 

------

top 100 bigrams, 

------

and top 100 trigrams in each data set. Finally, I looked at 

------

the distribution of the frequency of queries, words, bigrams, and trigrams in each system and each interface, 

------

and I looked at query length.

------

All right, so--no surprise here, but, generally speaking, I think my findings confirm that people *do* use these two systems differently and that overall the assumption that they understand they're working with different resource types in each one is valid--enough, I think, to help make an argument for bento box UI design.

------

We'll start with aggregate stats for each system. 

------

Sorry, I know this is probably hard to see. On top here we show catalog searches and on bottom we show Summon searches, from August 2011 to August 2014.

------

Here you can see where we implemented Summon as a beta service

------

and starting here is where Summon went live. So during this entire period, Summon and the catalog were both live, searchable via different tabs in our Find box. We didn't make any substantial changes to either interface during this time that would have much of an effect on these stats.

The first thing I want to note here, is that--

------

we see a dip in catalog usage here as we were ramping up Summon, 

------

but since Summon has been live, usage has equalized, and people have been using both systems in about equal amounts. In hindsight this might not be surprising, but, before we implemented Summon, we were a little curious whether or not Summon would cannibalize catalog use. Even after a couple of years, we see both a lot of search activity in Summon and people are still searching the catalog, which suggests that both systems are necessary if not that they outright fulfill different needs.

Next I want to point out the two clearly different patterns of use in each system by semester. 

------

Each long semester in the catalog (Fall and Spring) shows activity starting high and then falling,

------

and in Summon, we see exactly the opposite pattern, where usage peaks near the end of the semester. (NCSU has noted that they've seen a similar pattern.) We can't say for sure what exactly this pattern means, but we can say that it shows different use between the two systems.

Last thing I want to point out about this. 

------

You see these troughs that happen between semesters in each system. In Summon these are deeper and wider than 

------

they are in the catalog. When a semester ends, Summon use drops off immediately and remains low until the next semester begins, while catalog use remains a little bit steadier during semester breaks. (With the exception of Christmas/New Years.)

------

Now let's look at distance usage. 

------

This is actually showing local usage, using two different metrics for "local." It compares all searches (the red line) with searches coming from Denton, TX (where UNT is located, the blue line) and searches coming from the unt.edu domain ("on campus", the green line). Either one we look at, we see that the catalog gets substantially more use from local users than Summon.

------

In the catalog, 78.04% of use comes from Denton, and 57.29% of use comes from on-campus, 

------

while in Summon, 50.40% of use comes comes from Denton and only 25.38% of use comes from on-campus.

------

Next we'll look at usage of these two systems by where users started their search session. 

------

In this case it only makes sense to look at the time period after we redesigned the website, so August 2012 to August 2014.

Here we see that 75.34% of search sessions in Summon start from the Find Box, while only 40% of catalog search sessions do. Catalog search sessions are almost as likely to start from the catalog home page. Origination from other sources is also pretty high, I think because we have some other entrance points like course reserves, a scoped media search, and a scoped music search, which all see a fair amount of use.

------

Let's talk about queries. Queries are interesting to look at because they show better than simple usage stats what's going on in each system and each interface. The problem is that it's much harder to show you the data in a presentation format, because it's all lists of terms. So I'm just going to show you a small subset, and hopefully that will get my point across.

------

In the catalog,

------

looking at overall catalog searching, we see that the top queries tend to be searches related to video games, films, television, and books. We also see some searches in the catalog for particular resources--reference sources and serials especially. This is just the top 20, but that trend continues in the rest of the top 100.

------

If we look just at searches conducted via the Find Box, we see similar sorts of things, but heavier on actual books (mostly reference books) than video games, film, and television. We also see a couple of topical searches.

------

If we looked at bigrams and trigrams, which I'm not going to go through for the sake of brevity, we'd see more topics appear. Music, history, and literature (in addition to the pop culture terms we've seen) are more prominent.

------

In Summon, 

------

we mostly see queries related to academic topics or current events. We also see quite a few searches for databases or other types of aggregate resources,

------

especially when we look at the queries conducted via the Find Box.


Query analysis of course is tricky because there's a lot of data and a lot of variation. Looking at words, bigrams, and trigrams was one way to try to normalize and group queries. I've also attempted to characterize queries in two other ways: by looking at overall query length and by looking at search frequency distribution of the top 100 queries as a cumulative percentage of total searches.

So, this graph shows query length, looking at queries broken out by system and by Find Box vs all searches. We see that searches in the catalog tend to be shorter and searches in Summon tend to be longer. This jives with what others have found and I think maybe supports the idea that one prominent use case for Summon is to copy/paste an article title or citation to conduct known item searches.

This next set of graphs was an attempt to look at what percentage of total searches the top 100 queries, words, bigrams, and trigrams represent for each system. So: are the top 100 queries noticeably more heavily searched in one system compared to another?

And what we see is: for everything except single words, there is a noticeable--but not huge--difference between the catalog and Summon. The top 100 queries, or bigrams, or trigrams, that we see in the catalog represent a higher percentage of the total number of queries (bigrams, trigrams, etc.) than the top 100 queries etc. in Summon. This says to me that the distribution of searches in the catalog is slightly more heavily weighted toward the top queries, so people are tending to search a smaller set of queries more frequently in the catalog compared to Summon. And this pattern is even more pronounced when you look at the catalog queries entered into the Find Box.

All right. Sort of the last piece of the puzzle I want to complete is how people interact with Summon. I've mentioned that we primarily use Summon as a "find articles" search in our discovery environment. That search actually limits to the "journal article" content type by default. And in my analysis here, I'm basically equating use of Summon with use of articles. Is that a safe assumption, at least the majority of the time?

This is actually a little hard to answer because of the simple fact that people hardly ever change default settings. We see this over and over again in all kinds of UI research. So, in *our* stats, if we see that people rarely search things other than articles, is it because articles are actually what they want, or is it that they just don't want to bother changing the default?

Well, let's see what we see. Within Summon, 59.22% of searches never changed the default facets or filters at all, and 26.04% did. 8.26% of searches represent a new basic search (where facets/filters are completely reset), and 6.47% of searches were a new advanced search. Note that these categories are mutually exclusive, and that changes to facets/filters after a default Articles search or after a basic or advanced search are included in the 26.04% figure. So some of those facet/filter changes represent people adding the default filters back after they conducted a basic or advanced search that cleared out the default filters. (This is important to keep in mind.)

When we look at the types of facets and filters that are actually used, we see that changing content type is fifth, at 9.23% of facet/filter use. And of those, we're interested in seeing how people interact with journal articles. The top content type interaction is to add a limit for Journal Articles, at 21.94% of all content type interactions. The third is to remove the limit for Journal Articles, at 8.78%. Not even shown here because its use is so low is the Exclude Journal Articles filter, at 0.54%. Only 0.24% of total searches explicitly remove the Journal Articles content type limit, and 0.01% of total searches explicitly excluded journal articles. Finally, if we only look at the activity where people added a content type limit (as opposed to removing a limit or adding or removing an exclusion), we see that adding the Journal Articles limit accounts for 41.27% of all of that activity.

So here's what we can say for sure about this.

Number one, as we figured, people mostly don't mess with changing the defaults. (59.22% of total searches.)
Number two, even when people do change facets/filters, they rarely change content type. (1.89% of total searches.)
Number three, even when people do facets/filters, they rarely explicitly remove the Journal Article limit. (0.21% of total searches and only 0.91% of all facet/filter changes.)
Number four, people almost never actually exclude Journal Articles. (0.01% of total searches and 0.06% of all facet/filter changes.)
Number five, when people do change content type, adding the Journal Article content type limit back into their search is what we see most often. In absolute terms this is still rare, however you have to keep in mind that, since that limit is present on most searches by default, the opportunity to add the limit is a lot rarer than the opportunity to remove it or add other content type limits. 41.27% of all 

I think, while we can't say based on these numbers that people use Summon *exclusively* for searching articles, I think it does validate our assumption that people *primarily* use Summon for articles.

Okay let's wrap this up.

What we in libraries specifically refer to as the bento box interface basically started out as a hack to work around the particular aspects of the current system ecology that we find ourselves in. In most cases we're putting results from different systems into their own boxes, and we're shoehorning those results into categories by resource types that make sense to our users and best match up with what these systems contain. We're trying to create the illusion of one overarching library system and one consistent user experience instead of the pieced-together Frankenstein monsters that most of us actually have. Depending on our implementation, the places where the boundaries between systems don't quite line up with the categories we use in the UI I think is where we still have some problems in terms of meeting user expectations.

But the ultimate problems we're trying to solve with bento box UI design, once you get beyond the hackyness of any given implementation, I think are completely real and more universal than we might think. In the usage stats I've looked at, we see evidence that people do search for and use different types of resources differently. While we want to offer people the ease of a single-search box, we also want to make sure we don't accidentally make the things they want more difficult to find. Over the past couple of years, we've also been seeing the major search engines including more and more what we'd call bento box characteristics in their UIs. When Serials Solutions released Summon 2.0, we saw some of these same characteristics in their interface as well. Obviously there are differences in the way search engines implement these characteristics and the way that libraries typically have, but it seems to be the same attempt at bringing the most relevant and the widest variety of potentially relevant material to the top where users are likely to see it.




















